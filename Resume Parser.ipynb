{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b1e636",
   "metadata": {},
   "source": [
    "# LangExtract Resume Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a0e136",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec50731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "import json\n",
    "import webbrowser\n",
    "import pypdf\n",
    "import textwrap\n",
    "import io \n",
    "from typing import List\n",
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "\n",
    "\n",
    "from google import genai\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account \n",
    "\n",
    "import langextract as lx\n",
    "\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01044848",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38663adf",
   "metadata": {},
   "source": [
    "### Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad680b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load the variables from the .env file (it looks for a file named .env in the same directory)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c24a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf1a5833",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5eeed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGEXTRACT_API_KEY = os.environ.get(\"LANGEXTRACT_API_KEY\")\n",
    "\n",
    "GOOGLE_APPLICATION_CREDENTIALS = os.environ.get(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "\n",
    "BUCKET_NAME = os.environ.get(\"BUCKET_NAME\")\n",
    "\n",
    "PROJECT_ID = os.environ.get(\"PROJECT_ID\")\n",
    "\n",
    "TABLE_NAME = os.environ.get(\"TABLE_NAME\")\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "\n",
    "# Set pricing (as of late 2024, for input tokens)\n",
    "# Check the Google AI documentation for the most current pricing.\n",
    "PRICE_PER_MILLION_INPUT_TOKENS = 1.25  # Price in USD\n",
    "\n",
    "# URL valid for 30 minutes\n",
    "EXPIRATION = 30 \n",
    "\n",
    "# Define the target color for white text in PDF RGB (24-bit integer)\n",
    "# 0xFFFFFF in hex is 16777215 in decimal. This represents pure white foreground text.\n",
    "WHITE_COLOR_INT = 16777215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51da89e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d697153",
   "metadata": {},
   "source": [
    "### Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(LANGEXTRACT_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4290e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.models.get(model=MODEL_NAME).model_dump(exclude_defaults=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ca11c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "022f41e5",
   "metadata": {},
   "source": [
    "### GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dd459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_gcs_bucket_contents(bucket_name):\n",
    "    \"\"\"Lists all the blobs in the bucket.\"\"\"\n",
    "    \n",
    "    # Empty list\n",
    "    blob_list = []\n",
    "\n",
    "\n",
    "    # Initialize a storage client\n",
    "    storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # List all objects in the bucket\n",
    "    blobs = bucket.list_blobs()\n",
    "\n",
    "    print(\"Objects in GCS bucket:\")\n",
    "    for blob in blobs:\n",
    "        print(blob.name)\n",
    "        blob_list.append(blob.name)\n",
    "\n",
    "    return blob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabcbd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SECURITY NOTE (Best Practice) ---\n",
    "# For production environments (like Cloud Run or GKE), Google strongly recommends \n",
    "# avoiding downloaded Service Account key files and instead using Workload Identity \n",
    "# Federation or Service Account impersonation. This script uses the JSON key file \n",
    "# approach primarily for simple local testing where the environment is trusted \n",
    "# by the developer.\n",
    "\n",
    "#    Authentication (Local/Testing): V4 signed URLs require a Service Account key file (.json).\n",
    "#    Credentials provided by 'gcloud auth application-default login' are NOT enough.\n",
    "#    To run locally, you must set the environment variable to the path of your key file:\n",
    "#    export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/your/service-account-key.json\"\n",
    "\n",
    "def generate_gcs_signed_url(bucket_name: str, blob_name: str, expiration_minutes: int = 15, credentials_path: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Generates a V4 signed URL for a Google Cloud Storage object, allowing temporary\n",
    "    read access. This URL can be passed to the Gemini API for accessing data.\n",
    "\n",
    "    Args:\n",
    "        bucket_name: The name of the GCS bucket (e.g., 'my-media-bucket').\n",
    "        blob_name: The name of the object/file (e.g., 'images/input.png').\n",
    "        expiration_minutes: The duration (in minutes) the URL will be valid for.\n",
    "\n",
    "    Returns:\n",
    "        The generated signed URL string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # V4 signing requires credentials with a private key (Service Account).\n",
    "        \n",
    "        if credentials_path:\n",
    "            # Load explicit Service Account credentials for signing\n",
    "            credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "            storage_client = storage.Client(credentials=credentials)\n",
    "        else:\n",
    "            # Fallback to default credentials. This will only succeed if running\n",
    "            # on a Google Cloud service (like a VM or Cloud Run) with a Service \n",
    "            # Account already attached, or if default credentials include a key.\n",
    "            # If running locally without GOOGLE_APPLICATION_CREDENTIALS set, this \n",
    "            # will likely result in the 'private key' error.\n",
    "            storage_client = storage.Client()\n",
    "\n",
    "\n",
    "        # Get the bucket object\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "        # Get the blob (file) object\n",
    "        blob = bucket.blob(blob_name)\n",
    "        \n",
    "        # Calculate the expiration time\n",
    "        expiration_time = datetime.timedelta(minutes=expiration_minutes)\n",
    "\n",
    "        # Generate the signed URL using V4 signing\n",
    "        signed_url = blob.generate_signed_url(\n",
    "            version=\"v4\",\n",
    "            # The URL will be valid until this expiration time\n",
    "            expiration=expiration_time,\n",
    "            # The client needs 'GET' permission to read the file\n",
    "            method=\"GET\",\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ Generated signed URL for gs://{bucket_name}/{blob_name}\")\n",
    "        print(f\"   URL valid for {expiration_minutes} minutes.\")\n",
    "        return signed_url\n",
    "\n",
    "    except Exception as e:\n",
    "        # Check for the specific error to give a helpful message\n",
    "        if \"you need a private key to sign credentials\" in str(e):\n",
    "             print(\"‚ùå ERROR: V4 signing failed. Please ensure you have set the \"\n",
    "                   \"GOOGLE_APPLICATION_CREDENTIALS environment variable to the \"\n",
    "                   \"path of your Service Account JSON key file.\")\n",
    "             print(\"   Note: For production, we recommend more secure methods like \"\n",
    "                   \"Workload Identity Federation or Service Account impersonation.\")\n",
    "        else:\n",
    "            print(f\"‚ùå An unexpected error occurred during URL generation: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af97b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_list = list_gcs_bucket_contents(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f8ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOB_NAME = blob_list[1]\n",
    "BLOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2bd66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TEXT_URL = generate_gcs_signed_url(\n",
    "                                            bucket_name=BUCKET_NAME,\n",
    "                                            blob_name=BLOB_NAME,\n",
    "                                            expiration_minutes=EXPIRATION, \n",
    "                                            credentials_path=GOOGLE_APPLICATION_CREDENTIALS\n",
    "                                        )\n",
    "\n",
    "if TEXT_URL:\n",
    "    print(TEXT_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50387f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08226762",
   "metadata": {},
   "source": [
    "### Extract Text from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05838c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_url(pdf_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads a PDF file from a given URL by downloading its binary content into memory (BytesIO) \n",
    "    and extracts all text content without saving the file locally.\n",
    "    \n",
    "    Args:\n",
    "        pdf_url: The public or secured URL path to the PDF document (e.g., 'https://example.com/resume.pdf').\n",
    "\n",
    "    Returns:\n",
    "        A single string containing all text extracted from the PDF, or an empty string on error.\n",
    "    \"\"\"\n",
    "    \n",
    "    pdf_bytes = None\n",
    "    \n",
    "    # --- STEP 1: Fetch the Binary Content from the URL ---\n",
    "    try:\n",
    "        print(f\"Fetching PDF content from URL: {pdf_url}...\")\n",
    "        \n",
    "        # Use requests to get the content. Crucially, use .content for binary data (PDF)\n",
    "        response = requests.get(pdf_url)\n",
    "        response.raise_for_status()  # Check for HTTP errors (4xx or 5xx)\n",
    "        pdf_bytes = response.content # Use .content for binary data, NOT .text\n",
    "        \n",
    "        print(f\"Successfully fetched {len(pdf_bytes):,} bytes of PDF data.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching PDF from URL: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "    # Check if content was successfully downloaded\n",
    "    if not pdf_bytes:\n",
    "        print(\"Error: Downloaded content was empty.\")\n",
    "        return \"\"\n",
    "    \n",
    "    # --- STEP 2: Process Bytes in Memory ---\n",
    "    try:\n",
    "        # Create an in-memory file-like object from the bytes\n",
    "        pdf_stream = io.BytesIO(pdf_bytes)\n",
    "        \n",
    "        # Create a PdfReader object from the in-memory stream\n",
    "        reader = pypdf.PdfReader(pdf_stream)\n",
    "        full_text = []\n",
    "\n",
    "        # Loop through all pages and extract text\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                full_text.append(text)\n",
    "        \n",
    "        # Join the text from all pages with two newlines for separation\n",
    "        return \"\\n\\n\".join(full_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during PDF processing (in memory): {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c07c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the clean text from URL\n",
    "clean_resume_text = extract_text_from_url(TEXT_URL)\n",
    "\n",
    "# Print the result (you would pass this 'clean_resume_text' to lx.extract)\n",
    "print(f\"\\n--- Extracted Text from {TEXT_URL} (Preview) ---\")\n",
    "if clean_resume_text:\n",
    "    # Print the first 500 characters of the extracted text\n",
    "    print(textwrap.shorten(clean_resume_text, width=500, placeholder=\"... [Text Truncated]\"))\n",
    "    \n",
    "    # You would then use the clean_resume_text with your LangExtract definitions\n",
    "else:\n",
    "    print(\"No text was extracted or an error occurred. Ensure GCS authentication is set up and the URL is correct.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e17d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "760857b3",
   "metadata": {},
   "source": [
    "### Estimate Token Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Count the Tokens ---\n",
    "try:\n",
    "    print(f\"Initializing model: {MODEL_NAME}\")\n",
    "    model = client.models.get(model=MODEL_NAME)\n",
    "    \n",
    "    print(\"Counting tokens... (This may take a moment for a large book)\")\n",
    "    token_count = client.models.count_tokens(model=MODEL_NAME, contents=clean_resume_text)\n",
    "    \n",
    "    total_tokens = token_count.total_tokens\n",
    "    print(f\"\\n--- Results ---\")\n",
    "    print(f\"‚úÖ Total Tokens: {total_tokens:,}\")\n",
    "\n",
    "    # --- Estimate the Cost ---\n",
    "    # Cost = (Total Tokens / 1,000,000) * Price per Million\n",
    "    estimated_cost = (total_tokens / 1_000_000) * PRICE_PER_MILLION_INPUT_TOKENS\n",
    "    \n",
    "    print(f\"Price per 1M input tokens: ${PRICE_PER_MILLION_INPUT_TOKENS:.2f}\")\n",
    "    print(f\"üí∞ Estimated Cost (Input): ${estimated_cost:.6f}\") # Use .6f for small fractions of a cent\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while counting tokens: {e}\")\n",
    "    print(\"Please ensure your API key is correct and you have API access.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c15adfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea25a89d",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0813274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Prompt\n",
    "# Explicitly request all entities and structured/nested formats for complex ones.\n",
    "prompt = textwrap.dedent(\"\"\"\n",
    "    Extract the following entities from the text:\n",
    "    - Candidate Name\n",
    "    - Email Address\n",
    "    - Phone Number\n",
    "    - Skills (list)\n",
    "    - Education (nested structure)\n",
    "    - Work Experience (nested structure)\n",
    "    - Certifications (list)\n",
    "    - Projects (list)\n",
    "    - Languages Known (list)\n",
    "    - Awards and Honors (list)\n",
    "    \n",
    "    Provide the results in a structured JSON format. For Work Experience and Education, \n",
    "    extract each instance (e.g., a single job or degree) as a separate entity \n",
    "    using the 'attributes' field to capture details like dates, titles, and descriptions.\n",
    "\"\"\").strip()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50ffbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70cff1bd",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0343be90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a High-Quality Example (Few-Shot Prompting)\n",
    "# This example covers all requested fields, demonstrating both simple lists \n",
    "# and the complex, nested structures (Work Experience, Education) using attributes.\n",
    "\n",
    "examples: List[lx.data.ExampleData] = [\n",
    "    lx.data.ExampleData(\n",
    "        text=\"\"\"\n",
    "        Sam Tritto | sam.tritto@example.com | 555-123-4567\n",
    "\n",
    "        Experience\n",
    "        Data Scientist at The Home Depot (2019-2023). Led ML model deployment for inventory optimization.\n",
    "        Research Associate at University Y (2018-2019). Published 2 papers on NLP model efficiency.\n",
    "\n",
    "        Education\n",
    "        M.S. in Data Science, University X, Graduated 2019. Thesis on predictive analysis.\n",
    "\n",
    "        Skills: Python, TensorFlow, SQL, PyTorch.\n",
    "        Certifications: AWS Certified Data Analytics, Google Cloud Professional Data Engineer.\n",
    "        Projects: Recommendation Engine (GitHub link).\n",
    "        Languages: English (Fluent), Spanish (Conversational).\n",
    "        Awards: Employee of the Year 2021 (THD).\n",
    "        \"\"\",\n",
    "        extractions=[\n",
    "            # --- Simple Contact & Identifier Entities ---\n",
    "            lx.data.Extraction(extraction_class=\"candidate_name\", extraction_text=\"Sam Tritto\"),\n",
    "            lx.data.Extraction(extraction_class=\"email_address\", extraction_text=\"sam.tritto@example.com\"),\n",
    "            lx.data.Extraction(extraction_class=\"phone_number\", extraction_text=\"555-123-4567\"),\n",
    "\n",
    "            # --- List Entities (Repeat extraction_class for each item) ---\n",
    "            lx.data.Extraction(extraction_class=\"skill\", extraction_text=\"Python\"),\n",
    "            lx.data.Extraction(extraction_class=\"skill\", extraction_text=\"TensorFlow\"),\n",
    "            lx.data.Extraction(extraction_class=\"skill\", extraction_text=\"SQL\"),\n",
    "            lx.data.Extraction(extraction_class=\"skill\", extraction_text=\"PyTorch\"),\n",
    "            \n",
    "            lx.data.Extraction(extraction_class=\"language\", extraction_text=\"English (Fluent)\"),\n",
    "            lx.data.Extraction(extraction_class=\"language\", extraction_text=\"Spanish (Conversational)\"),\n",
    "\n",
    "            lx.data.Extraction(extraction_class=\"certification\", extraction_text=\"AWS Certified Data Analytics\"),\n",
    "            lx.data.Extraction(extraction_class=\"certification\", extraction_text=\"Google Cloud Professional Data Engineer\"),\n",
    "\n",
    "            lx.data.Extraction(extraction_class=\"project\", extraction_text=\"Recommendation Engine (GitHub link)\"),\n",
    "\n",
    "            lx.data.Extraction(extraction_class=\"award\", extraction_text=\"Employee of the Year 2021 (THD)\"),\n",
    "\n",
    "            # --- Complex/Nested Entity: Work Experience (Instance 1) ---\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"work_experience\",\n",
    "                extraction_text=\"Data Scientist at The Home Depot (2019-2023). Led ML model deployment for inventory optimization.\",\n",
    "                attributes={\n",
    "                    \"role\": \"Data Scientist\",\n",
    "                    \"company\": \"The Home Depot\",\n",
    "                    \"start_year\": \"2019\",\n",
    "                    \"end_year\": \"2023\",\n",
    "                    \"description\": \"Led ML model deployment for inventory optimization.\"\n",
    "                }\n",
    "            ),\n",
    "            # --- Complex/Nested Entity: Work Experience (Instance 2) ---\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"work_experience\",\n",
    "                extraction_text=\"Research Associate at University Y (2018-2019). Published 2 papers on NLP model efficiency.\",\n",
    "                attributes={\n",
    "                    \"role\": \"Research Associate\",\n",
    "                    \"company\": \"University Y\",\n",
    "                    \"start_year\": \"2018\",\n",
    "                    \"end_year\": \"2019\",\n",
    "                    \"description\": \"Published 2 papers on NLP model efficiency.\"\n",
    "                }\n",
    "            ),\n",
    "            \n",
    "            # --- Complex/Nested Entity: Education ---\n",
    "            lx.data.Extraction(\n",
    "                extraction_class=\"education\",\n",
    "                extraction_text=\"M.S. in Data Science, University X, Graduated 2019. Thesis on predictive analysis.\",\n",
    "                attributes={\n",
    "                    \"degree\": \"M.S. in Data Science\",\n",
    "                    \"institution\": \"University X\",\n",
    "                    \"graduation_year\": \"2019\",\n",
    "                    \"description\": \"Thesis on predictive analysis.\"\n",
    "                }\n",
    "            ),\n",
    "            \n",
    "        ]\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcfd175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64f78003",
   "metadata": {},
   "source": [
    "### LangExtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c731cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = lx.extract(\n",
    "    text_or_documents=clean_resume_text,\n",
    "    prompt_description=prompt,\n",
    "    examples=examples,\n",
    "    model_id=MODEL_NAME,\n",
    "    extraction_passes=2,    # Improves recall through multiple passes\n",
    "    max_workers=20,         # Parallel processing for speed\n",
    "    max_char_buffer=1000    # Smaller contexts for better accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52adb11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a JSONL file\n",
    "lx.io.save_annotated_documents([result], output_name=\"extraction_results.jsonl\", output_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843355b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63cfb52e",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea64c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"visualization.html\"\n",
    "\n",
    "# Generate the visualization from the file\n",
    "html_content = lx.visualize(\"extraction_results.jsonl\")\n",
    "with open(file_name, \"w\") as f:\n",
    "    if hasattr(html_content, 'data'):\n",
    "        f.write(html_content.data)  # For Jupyter/Colab\n",
    "    else:\n",
    "        f.write(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36355e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def open_html_in_browser(file_path: str):\n",
    "    \"\"\"\n",
    "    Opens a local HTML file in the default web browser.\n",
    "    \n",
    "    Args:\n",
    "        file_path: The local path to the HTML file.\n",
    "    \"\"\"\n",
    "    # Convert the file path to a URL format (needed for cross-platform reliability)\n",
    "    # The 'file:///' prefix tells the browser it's a local file.\n",
    "    full_path = os.path.abspath(file_path)\n",
    "    webbrowser.open_new_tab(f\"file:///{full_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e81f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the generated file in your browser\n",
    "open_html_in_browser(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d548c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98ccc540",
   "metadata": {},
   "source": [
    "### White Text Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3823b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_white_text(pdf_file_path_or_bytes: str | bytes) -> dict:\n",
    "    \"\"\"\n",
    "    Scans a PDF document for text spans where the foreground color is white\n",
    "    (0xFFFFFF), indicating potentially hidden text against a white background.\n",
    "\n",
    "    Args:\n",
    "        pdf_file_path_or_bytes: The file path (str) or file bytes (bytes)\n",
    "                                of the PDF resume.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing a list of findings and a summary flag.\n",
    "    \"\"\"\n",
    "    findings = []\n",
    "    \n",
    "    # Handle both file paths (for local testing) and bytes (for Streamlit upload)\n",
    "    try:\n",
    "        if isinstance(pdf_file_path_or_bytes, bytes):\n",
    "            # Open from memory (e.g., Streamlit uploaded file)\n",
    "            doc = fitz.open(stream=pdf_file_path_or_bytes, filetype=\"pdf\")\n",
    "        else:\n",
    "            # Open from file path\n",
    "            doc = fitz.open(pdf_file_path_or_bytes)\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening PDF: {e}\")\n",
    "        return {\"flagged\": False, \"findings\": [], \"error\": str(e)}\n",
    "\n",
    "    for page_num, page in enumerate(doc):\n",
    "        # Use 'dict' output for maximum detail, down to the span level\n",
    "        text_data = page.get_text('dict')\n",
    "        \n",
    "        for block in text_data.get('blocks', []):\n",
    "            if 'lines' in block:\n",
    "                for line in block['lines']:\n",
    "                    for span in line['spans']:\n",
    "                        # PyMuPDF stores color as a 24-bit integer\n",
    "                        span_color = span.get('color')\n",
    "                        \n",
    "                        # Check if the text color is white\n",
    "                        if span_color == WHITE_COLOR_INT:\n",
    "                            # Extract the text and its bounding box for reporting\n",
    "                            hidden_text = span['text'].strip()\n",
    "                            bbox = span['bbox'] # (x0, y0, x1, y1)\n",
    "                            \n",
    "                            if hidden_text: # Only log non-empty strings\n",
    "                                findings.append({\n",
    "                                    \"page\": page_num + 1,\n",
    "                                    \"text\": hidden_text,\n",
    "                                    \"bbox\": bbox,\n",
    "                                    \"reason\": \"Foreground text color is white (0xFFFFFF).\"\n",
    "                                })\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    return {\n",
    "        \"flagged\": len(findings) > 0,\n",
    "        \"findings\": findings,\n",
    "        \"total_findings\": len(findings)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5e7228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with a local file (for testing)\n",
    "white_text_results = detect_white_text('Resume (white text).pdf')\n",
    "print(white_text_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9179e6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a244dc1b",
   "metadata": {},
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lx_results_from_jsonl(filepath: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Loads one or more LangExtract document results from a JSON Lines (.jsonl) file.\n",
    "    Each line in the file is expected to be a valid JSON object representing a document.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                # Process only non-empty lines\n",
    "                if line.strip():\n",
    "                    try:\n",
    "                        # Parse the JSON string from the line\n",
    "                        results.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error decoding JSON line: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{filepath}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while reading the file: {e}\")\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9474b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_lx_output_to_dataframe(lx_result: dict, file_name: str, white_text_results: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes the structured JSON output from a LangExtract (lx.extract) call \n",
    "    and converts it into a clean pandas DataFrame.\n",
    "\n",
    "    This function handles both simple fields (like name, email, skills) and \n",
    "    nested fields (like work_experience with attributes).\n",
    "    \n",
    "    Args:\n",
    "        lx_result: The dictionary output representing the full document extraction.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame where each row is a single extracted entity.\n",
    "    \"\"\"\n",
    "    \n",
    "    records = []\n",
    "    \n",
    "    # ---------------------------------------------------\n",
    "    # Process the LangExtract data\n",
    "    # ---------------------------------------------------\n",
    "\n",
    "    # Iterate through all extracted entities\n",
    "    for extraction in lx_result.get(\"extractions\", []):\n",
    "        class_name = extraction.get(\"extraction_class\")\n",
    "        text_value = extraction.get(\"extraction_text\")\n",
    "        attributes = extraction.get(\"attributes\", {})\n",
    "        \n",
    "        # Start the record with basic details\n",
    "        record = {\n",
    "            \"entity_type\": class_name,\n",
    "            \"raw_text_value\": text_value,\n",
    "        }\n",
    "        \n",
    "        # If there are nested attributes, add them to the record.\n",
    "        # This is where we flatten the nested structure.\n",
    "        if attributes:\n",
    "            # For complex entities, the desired value is usually in the attributes\n",
    "            # rather than the raw text. We merge the attributes directly.\n",
    "            record.update(attributes)\n",
    "        else:\n",
    "            # For simple entities (Name, Email, Skills), the value is the raw_text_value itself.\n",
    "            record[\"value\"] = text_value\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Process the hidden white text findings\n",
    "    # ---------------------------------------------------\n",
    "\n",
    "    try:\n",
    "        # parsed_json = json.loads(white_text_results)\n",
    "        parsed_json = white_text_results\n",
    "        findings = parsed_json.get(\"findings\", [])\n",
    "        for finding in findings:\n",
    "            # Create a standardized record for each finding\n",
    "            record = {\n",
    "                # New entity type for the final structured finding\n",
    "                \"entity_type\": \"hidden_text_finding\",\n",
    "                \"raw_text_value\": finding.get(\"text\", \"N/A\"),\n",
    "                \"hidden_page\": finding.get(\"page\", \"N/A\"),\n",
    "                \"hidden_reason\": finding.get(\"reason\", \"N/A\"),\n",
    "                # Convert list bbox to string for easier display in DataFrame\n",
    "                \"hidden_bbox\": str(finding.get(\"bbox\", \"N/A\"))\n",
    "            }\n",
    "            records.append(record)\n",
    "        print(f\"Successfully parsed {len(findings)} structured hidden text findings.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        # Handle case where LLM or mock injected malformed JSON\n",
    "        print(f\"‚ùå Error parsing Raw Hidden Text JSON: {e}\")\n",
    "        records.append({\n",
    "            \"entity_type\": \"raw_hidden_text_json_error\", \n",
    "            \"raw_text_value\": white_text_results, \n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    \n",
    "    # Optional: Clean up and reorder columns for a clearer view\n",
    "    # This ensures columns like 'role', 'company', etc., show up nicely.\n",
    "    df = df.sort_values(by='entity_type').reset_index(drop=True)\n",
    "\n",
    "    df['file_name'] = file_name \n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b88a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lx_data = load_lx_results_from_jsonl(\"extraction_results.jsonl\")[0]\n",
    "extraction_df = process_lx_output_to_dataframe(lx_data, BLOB_NAME, white_text_results)\n",
    "extraction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7577d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43a37e52",
   "metadata": {},
   "source": [
    "### BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d5835",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_df.to_gbq(\n",
    "                        destination_table=TABLE_NAME,\n",
    "                        project_id=PROJECT_ID,\n",
    "                        if_exists=\"append\" \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645256af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe5c93d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
